{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "from keras import regularizers\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import os, shutil\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_dir: data/cats_and_dogs_small already exist\n",
      "no sub-directories will be created\n"
     ]
    }
   ],
   "source": [
    "# importing dataset\n",
    "\n",
    "original_dataset_dir = 'data/PetImages'\n",
    "\n",
    "# subset directories to store images\n",
    "\n",
    "# directory names\n",
    "\n",
    "base_dir = 'data/cats_and_dogs_small'\n",
    "\n",
    "train_dir = os.path.join(base_dir,'train')\n",
    "train_cats_dir = os.path.join(train_dir,'cats')\n",
    "train_dogs_dir = os.path.join(train_dir,'dogs')\n",
    "\n",
    "validation_dir = os.path.join(base_dir,'validation')\n",
    "validation_cats_dir = os.path.join(validation_dir,'cats')\n",
    "validation_dogs_dir = os.path.join(validation_dir,'dogs')\n",
    "\n",
    "test_dir = os.path.join(base_dir,'test')\n",
    "test_cats_dir = os.path.join(test_dir,'cats')\n",
    "test_dogs_dir = os.path.join(test_dir,'dogs')\n",
    "\n",
    "try:\n",
    "    # creating directories: \n",
    "    \n",
    "    os.mkdir(base_dir)\n",
    "    \n",
    "    os.mkdir(train_dir)    \n",
    "    os.mkdir(train_cats_dir)\n",
    "    os.mkdir(train_dogs_dir)        \n",
    "    \n",
    "    os.mkdir(validation_dir)    \n",
    "    os.mkdir(validation_cats_dir)    \n",
    "    os.mkdir(validation_dogs_dir)\n",
    "    \n",
    "    \n",
    "    os.mkdir(test_dir)    \n",
    "    os.mkdir(test_cats_dir)    \n",
    "    os.mkdir(test_dogs_dir)\n",
    "    \n",
    "    for i in range(1000):\n",
    "        # training sample\n",
    "        \n",
    "        file_name = str(i)+'.jpg'\n",
    "        \n",
    "        # for cats\n",
    "        src = os.path.join(original_dataset_dir,'Cat',file_name)\n",
    "        dst = os.path.join(train_cats_dir,file_name)\n",
    "        shutil.copyfile(src,dst)\n",
    "        \n",
    "        #for dogs\n",
    "        src = os.path.join(original_dataset_dir,'Dog',file_name)\n",
    "        dst = os.path.join(train_dogs_dir,file_name)\n",
    "        shutil.copyfile(src,dst)\n",
    "        \n",
    "    for i in range(1000,1500):\n",
    "        \n",
    "        # validation sample\n",
    "        \n",
    "        file_name = str(i)+'.jpg'\n",
    "        \n",
    "        # for cats\n",
    "        src = os.path.join(original_dataset_dir,'Cat',file_name)\n",
    "        dst = os.path.join(validation_cats_dir,file_name)\n",
    "        shutil.copyfile(src,dst)\n",
    "        \n",
    "        #for dogs\n",
    "        src = os.path.join(original_dataset_dir,'Dog',file_name)\n",
    "        dst = os.path.join(validation_dogs_dir,file_name)\n",
    "        shutil.copyfile(src,dst)\n",
    "        \n",
    "        # testing sample\n",
    "        \n",
    "        j = i + 500\n",
    "        \n",
    "        file_name = str(j)+'.jpg'\n",
    "        \n",
    "        # for cats\n",
    "        src = os.path.join(original_dataset_dir,'Cat',file_name)\n",
    "        dst = os.path.join(test_cats_dir,file_name)\n",
    "        shutil.copyfile(src,dst)\n",
    "        \n",
    "        #for dogs\n",
    "        src = os.path.join(original_dataset_dir,'Dog',file_name)\n",
    "        dst = os.path.join(test_dogs_dir,file_name)\n",
    "        shutil.copyfile(src,dst)\n",
    "        \n",
    "    \n",
    "    \n",
    "except FileExistsError:\n",
    "    print('base_dir:',base_dir,'already exist')\n",
    "    print('no sub-directories will be created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Cat images\n",
      "training images:  1000\n",
      "validation images:  500\n",
      "testing images:  500\n",
      "\n",
      "Total Dog images \n",
      "training images:  1000\n",
      "validation images:  500\n",
      "testing images:  500\n"
     ]
    }
   ],
   "source": [
    "# check number of images in each sample\n",
    "print('Total Cat images')\n",
    "print('training images: ',len(os.listdir(train_cats_dir)))\n",
    "print('validation images: ',len(os.listdir(validation_cats_dir)))\n",
    "print('testing images: ', len(os.listdir(test_cats_dir)))\n",
    "print('')\n",
    "print('Total Dog images ')\n",
    "print('training images: ',len(os.listdir(train_dogs_dir)))\n",
    "print('validation images: ',len(os.listdir(validation_dogs_dir)))\n",
    "print('testing images: ', len(os.listdir(test_dogs_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 72, 72, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               3211776   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 3,453,121\n",
      "Trainable params: 3,453,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiating a small convnet\n",
    "\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(filters=32,\n",
    "                        kernel_size=[3,3],\n",
    "                        activation='relu',\n",
    "                        input_shape=[150,150,3]))\n",
    "model.add(layers.MaxPool2D(pool_size=[2,2]))\n",
    "\n",
    "model.add(layers.Conv2D(filters=64,\n",
    "                        kernel_size=[3,3],\n",
    "                        activation='relu'))\n",
    "model.add(layers.MaxPool2D(pool_size=[2,2]))\n",
    "\n",
    "model.add(layers.Conv2D(filters=128,\n",
    "                        kernel_size=[3,3],\n",
    "                        activation='relu'))\n",
    "model.add(layers.MaxPool2D(pool_size=[2,2]))\n",
    "\n",
    "model.add(layers.Conv2D(filters=128,\n",
    "                        kernel_size=[3,3],\n",
    "                        activation='relu'))\n",
    "model.add(layers.MaxPool2D(pool_size=[2,2]))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure the model for training\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0001"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading images\n",
    "\n",
    "# rescaling all images by 1/255\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                    target_size=[150,150]\n",
    "                                                    batch_size=20,\n",
    "                                                    class_mode='binary') # binary labels\n",
    "validation_generator = test_datagen.flow_from_directory(validation_dir,\n",
    "                                                        target_size=[150,150],\n",
    "                                                        batch_size=20,\n",
    "                                                        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
